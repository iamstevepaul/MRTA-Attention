import torchimport numpy as npimport timefrom torch import nnimport randomclass GCN(nn.Module):    def __init__(self,                 n_layers = 3,                 n_hops = 5,                 n_dim = 128,                 n_p = 3,                 node_dim = 3                 ):        super(GCN, self).__init__()        self.n_layers = n_layers        self.n_hops = n_hops        self.n_dim = n_dim        self.n_p = n_p        self.node_dim = node_dim        self.init_embed = nn.Linear(node_dim, n_dim)        self.W1 = nn.Linear(n_dim*n_p, n_dim)        self.W2 = nn.Linear(n_dim * n_p, n_dim)        self.activ = nn.LeakyReLU()    def forward(self, X, mask=None):        # X = torch.cat((data['loc'], data['deadline']), -1)        X_loc = X[:, :, 0:2]        distance_matrix = (((X_loc[:, :, None] - X_loc[:, None]) ** 2).sum(-1)) ** .5        A = (distance_matrix < .3).to(torch.int8)        num_samples, num_locations, _ = X.size()        D = torch.mul(torch.eye(num_locations).expand((num_samples, num_locations, num_locations)),                      (A.sum(-1) - 1)[:, None].expand((num_samples, num_locations, num_locations)))        L = D - A        F0 = self.init_embed(X)        g1 = torch.cat((F0[:,:,:,None], torch.matmul(L,F0)[:,:,:,None], torch.matmul(torch.matmul(L,L), F0)[:,:,:,None]), -1).reshape((num_samples, num_locations, -1))        F1 =   self.activ(self.W1(g1))        g2 = torch.cat((F1[:,:,:,None], torch.matmul(L,F1)[:,:,:,None], torch.matmul(torch.matmul(L,L), F1)[:,:,:,None]), -1).reshape((num_samples, num_locations, -1))        return self.activ(self.W2(g2))class GCAPCN(nn.Module):    def __init__(self,                 n_layers = 2,                 n_dim = 128,                 n_p = 3,                 node_dim = 3,                 n_K = 1                 ):        super(GCAPCN, self).__init__()        self.n_layers = n_layers        self.n_dim = n_dim        self.n_p = n_p        self.n_K = n_K        self.node_dim = node_dim        self.init_embed = nn.Linear(node_dim, n_dim*n_p)        self.init_embed_depot = nn.Linear(2, n_dim)        self.W_L_1_G1 = nn.Linear(n_dim * (n_K+1)*n_p, n_dim)        self.W_L_1_G2 = nn.Linear(n_dim * (n_K+1)*n_p, n_dim)        self.W_L_1_G3 = nn.Linear(n_dim * (n_K+1)*n_p, n_dim)        self.W_L_2_G1 = nn.Linear(n_dim * (n_K + 1)*n_p, n_dim)        self.W_L_2_G2 = nn.Linear(n_dim * (n_K + 1)*n_p, n_dim)        self.W_L_2_G3 = nn.Linear(n_dim * (n_K + 1)*n_p, n_dim)        self.normalization_1 = nn.BatchNorm1d(n_dim * n_p)        self.normalization_2 = nn.BatchNorm1d(n_dim * n_p)        self.W_F = nn.Linear(n_dim * n_p, n_dim)        self.activ = nn.LeakyReLU()    def forward(self, X, mask=None):        # X = torch.cat((data['loc'], data['deadline']), -1)        X_loc = X[:, :, 0:2]        distance_matrix = (((X_loc[:, :, None] - X_loc[:, None]) ** 2).sum(-1)) ** .5        A = distance_matrix        num_samples, num_locations, _ = X.size()        D = torch.mul(torch.eye(num_locations).expand((num_samples, num_locations, num_locations)),                      (A.sum(-1) - 1)[:, None].expand((num_samples, num_locations, num_locations)))        # Layer 1        # p = 3        F0 = self.init_embed(X)        F0_squared = torch.mul(F0[:, :, :], F0[:, :, :])        F0_cube = torch.mul(F0[:, :, :], F0_squared[:, :, :])        # K = 1        L = D - A        # L_squared = torch.matmul(L, L)        # L_cube = torch.matmul(L, L_squared)        g_L1_1 = self.W_L_1_G1(torch.cat((F0[:, :, :],                          torch.matmul(L, F0)[:, :, :]                          # torch.matmul(L_squared, F0)[:, :, :],                          # torch.matmul(L_cube, F0)[:, :, :]                          ),                         -1))        g_L1_2 = self.W_L_1_G2(torch.cat((F0_squared[:, :, :],                                        torch.matmul(L, F0_squared)[:, :, :]                                        # torch.matmul(L_squared, F0_squared)[:, :, :],                                        # torch.matmul(L_cube, F0_squared)[:, :, :]                                        ),                                       -1))        g_L1_3 = self.W_L_1_G3(torch.cat((F0_cube[:, :, :],                                          # torch.matmul(L, F0_cube)[:, :, :]                                          # torch.matmul(L_squared, F0_cube)[:, :, :],                                          # torch.matmul(L_cube, F0_cube)[:, :, :]                                          ),                                         -1))        F1 = torch.cat((g_L1_1, g_L1_2, g_L1_3), -1)        F1 = self.activ(F1) + F0        F1 = batch_norm(F1)        # # Layer 2        #        # F1_squared = torch.mul(F1[:, :, :], F1[:, :, :])        # F1_cube = torch.mul(F1[:, :, :], F1[:, :, :])        # g_L2_1 = self.W_L_2_G1(torch.cat((F1[:, :, :],        #                                   torch.matmul(L, F1)[:, :, :],        #                                   torch.matmul(L_squared, F1)[:, :, :],        #                                   torch.matmul(L_cube, F1)[:, :, :]        #                                   ),        #                                  -1))        # g_L2_2 = self.W_L_2_G2(torch.cat((F1_squared[:, :, :],        #                                   torch.matmul(L, F1_squared)[:, :, :],        #                                   torch.matmul(L_squared, F1_squared)[:, :, :],        #                                   torch.matmul(L_cube, F1_squared)[:, :, :]        #                                   ),        #                                  -1))        #        # g_L2_3 = self.W_L_2_G3(torch.cat((F1_cube[:, :, :],        #                                   torch.matmul(L, F1_cube)[:, :, :],        #                                   torch.matmul(L_squared, F1_cube)[:, :, :],        #                                   torch.matmul(L_cube, F1_cube)[:, :, :]        #                                   ),        #                                  -1))        #        # F2 = torch.cat((g_L2_1, g_L2_2, g_L2_3), -1)        # F2 = self.activ(F2) + F1        # F2 = batch_norm(F2)        F_final = self.activ(self.W_F(F1))        init_depot_embed = self.init_embed_depot(data['depot'])[:, None]        h = torch.cat((init_depot_embed, F_final), 1)        return F_finaldef batch_norm(F):    return ((F.view(-1, F.size(-1)) - F.view(-1, F.size(-1)).mean(-2)) / (                F.view(-1, F.size(-1)).std(-2) + 0.00001)).view(*F.size())class GCAPCN_WR(nn.Module):    def __init__(self,                 n_layers = 3,                 n_hops = 5,                 n_dim = 128,                 n_p = 2,                 node_dim = 3                 ):        super(GCAPCN, self).__init__()        self.n_layers = n_layers        self.n_hops = n_hops        self.n_dim = n_dim        self.n_p = n_p        self.node_dim = node_dim        self.init_embed = nn.Linear(node_dim, n_dim)        self.W1 = nn.Linear(n_dim*n_p, n_dim)        self.W2 = nn.Linear(n_dim * n_p, n_dim)        self.activ = nn.LeakyReLU()        self.normalization_1 = nn.BatchNorm1d(n_dim * n_p)        self.normalization_2 = nn.BatchNorm1d(n_dim)        self.normalization_3 = nn.BatchNorm1d(n_dim * n_p)        self.normalization_4 = nn.BatchNorm1d(n_dim)    def forward(self, X, mask=None):        # X = torch.cat((data['loc'], data['deadline']), -1)        X_loc = X[:, :, 0:2]        distance_matrix = (((X_loc[:, :, None] - X_loc[:, None]) ** 2).sum(-1)) ** .5        A = distance_matrix #(distance_matrix < 1.5).to(torch.int8)        num_samples, num_locations, _ = X.size()        D = torch.mul(torch.eye(num_locations).expand((num_samples, num_locations, num_locations)),                      (A.sum(-1) - 1)[:, None].expand((num_samples, num_locations, num_locations)))        L = D - A        F0 = self.init_embed(X)        g1_1 = torch.cat((F0[:, :, :, None], torch.matmul(L, F0)[:, :, :, None],                          torch.matmul(torch.matmul(L, L), F0)[:, :, :, None]), -1).sum(-1)        g1_2 = torch.cat((torch.mul(F0[:, :, :, None], F0[:, :, :, None]),                          torch.matmul(L, torch.mul(F0[:, :, :], F0[:, :, :]))[:, :, :, None],                          torch.matmul(torch.matmul(L, L), torch.mul(F0[:, :, :], F0[:, :, :]))[:, :, :, None]),                         -1).sum(-1)        g1_cat = torch.cat((g1_1, g1_2), -1)        inter_1 = self.normalization_1(g1_cat.view(-1, g1_cat.size(-1))).view(*g1_cat.size())        F1 =   self.activ(self.W1(inter_1)) + F0 # F0 for skip connection        F1_norm = self.normalization_2(F1.view(-1, F1.size(-1))).view(*F1.size())        g2_1 = torch.cat((F1_norm[:, :, :, None], torch.matmul(L, F1_norm)[:, :, :, None],                          torch.matmul(torch.matmul(L, L), F1_norm)[:, :, :, None]), -1).sum(-1)        g2_2 = torch.cat((torch.mul(F1_norm[:, :, :, None], F1_norm[:, :, :, None]),                          torch.matmul(L, torch.mul(F1_norm[:, :, :], F1_norm[:, :, :]))[:, :, :, None],                          torch.matmul(torch.matmul(L, L), torch.mul(F1_norm[:, :, :], F1_norm[:, :, :]))[:, :, :,                          None]),                         -1).sum(-1)        g2_cat = torch.cat((g2_1, g2_2), -1)        inter_2 = self.normalization_3(g2_cat.view(-1, g2_cat.size(-1))).view(*g2_cat.size())        F2 = self.activ(self.W2(inter_2)) + F1 # F11 for skip connection        F2_norm = self.normalization_4(F2.view(-1, F2.size(-1))).view(*F2.size())        return F2_normif __name__ == '__main__':    num_locations = 100    new_index = torch.tensor(random.sample(range(0, 100), 100))        num_samples = 2    data = {        'loc': torch.FloatTensor(num_samples, num_locations, 2).uniform_(0, 1),        'depot': torch.FloatTensor(num_samples ,2).uniform_(0, 1),        'deadline': torch.FloatTensor(num_samples, num_locations, 1).uniform_(0.1, 1)    }    for i in range(100):        ind = new_index[i]        data['loc'][1, i,:] = data['loc'][0, ind,:]        data['deadline'][1, i, :] = data['deadline'][0, ind, :]    X = torch.cat((data['loc'], data['deadline']), -1)    enc = GCAPCN()    embed = enc(X)    check = 0    print(embed[0, new_index[check],:] - embed[1, check,:])    print((embed[0, new_index[check],:] - embed[1, check,:]).norm())    check = 1    print(embed[0, new_index[check], :] - embed[1, check, :])    print((embed[0, new_index[check], :] - embed[1, check, :]).norm())    check = 2    print(embed[0, new_index[check], :] - embed[1, check, :])    print((embed[0, new_index[check], :] - embed[1, check, :]).norm())